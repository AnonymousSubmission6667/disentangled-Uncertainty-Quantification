{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-benchmark using fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# to remplace with pip install abench and uqmodels\n",
    "path = '../'\n",
    "sys.path.insert(1, path)\n",
    "sys.path.insert(1, path+'src')\n",
    "import abench as abench\n",
    "import uqmodels as uqmodels\n",
    "import abench.store as A_store\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "# Check that current directory is at root level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing and setup as Data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/Benchmark_real_injection_learning already exists\n",
      "% train for each sub sample for each set\n",
      "cv_0  len 8473 %train/test/drop: [22.  43.1 17.1] [4.9 9.2 3.7] [0. 0. 0.]\n",
      "cv_1  len 10297 %train/test/drop: [22.2 43.  17.1] [4.9 9.2 3.7] [0. 0. 0.]\n",
      "cv_0_bis1  len 8473 %train/test/drop: [22.  43.1 17.1] [4.9 9.2 3.7] [0. 0. 0.]\n",
      "cv_1_bis1  len 10297 %train/test/drop: [22.2 43.  17.1] [4.9 9.2 3.7] [0. 0. 0.]\n",
      "cv_0_deg0_seq100  len 8473 %train/test/drop: [ 0.  43.1 17.1] [4.9 9.2 3.7] [22.  0.  0.]\n",
      "cv_1_deg0_seq100  len 10297 %train/test/drop: [ 0.  43.  17.1] [4.9 9.2 3.7] [22.2  0.   0. ]\n",
      "cv_0_deg0_seq100_bis1  len 8473 %train/test/drop: [ 0.  43.1 17.1] [4.9 9.2 3.7] [22.  0.  0.]\n",
      "cv_1_deg0_seq100_bis1  len 10297 %train/test/drop: [ 0.  43.  17.1] [4.9 9.2 3.7] [22.2  0.   0. ]\n",
      "cv_0_deg1_seq100  len 8473 %train/test/drop: [22.   0.  17.1] [4.9 9.2 3.7] [ 0.  43.1  0. ]\n",
      "cv_1_deg1_seq100  len 10297 %train/test/drop: [22.2  0.  17.1] [4.9 9.2 3.7] [ 0. 43.  0.]\n",
      "cv_0_deg1_seq100_bis1  len 8473 %train/test/drop: [22.   0.  17.1] [4.9 9.2 3.7] [ 0.  43.1  0. ]\n",
      "cv_1_deg1_seq100_bis1  len 10297 %train/test/drop: [22.2  0.  17.1] [4.9 9.2 3.7] [ 0. 43.  0.]\n",
      "cv_0_deg2_seq100  len 8473 %train/test/drop: [22.  43.1  0. ] [4.9 9.2 3.7] [ 0.   0.  17.1]\n",
      "cv_1_deg2_seq100  len 10297 %train/test/drop: [22.2 43.   0. ] [4.9 9.2 3.7] [ 0.   0.  17.1]\n",
      "cv_0_deg2_seq100_bis1  len 8473 %train/test/drop: [22.  43.1  0. ] [4.9 9.2 3.7] [ 0.   0.  17.1]\n",
      "cv_1_deg2_seq100_bis1  len 10297 %train/test/drop: [22.2 43.   0. ] [4.9 9.2 3.7] [ 0.   0.  17.1]\n",
      "cv_0_deg0_seq98  len 8473 %train/test/drop: [ 0.2 43.1 17.1] [4.9 9.2 3.7] [21.8  0.   0. ]\n",
      "cv_1_deg0_seq98  len 10297 %train/test/drop: [ 0.2 43.  17.1] [4.9 9.2 3.7] [22.  0.  0.]\n",
      "cv_0_deg0_seq98_bis1  len 8473 %train/test/drop: [ 0.2 43.1 17.1] [4.9 9.2 3.7] [21.8  0.   0. ]\n",
      "cv_1_deg0_seq98_bis1  len 10297 %train/test/drop: [ 0.2 43.  17.1] [4.9 9.2 3.7] [22.  0.  0.]\n",
      "cv_0_deg1_seq98  len 8473 %train/test/drop: [22.   2.5 17.1] [4.9 9.2 3.7] [ 0.  40.6  0. ]\n",
      "cv_1_deg1_seq98  len 10297 %train/test/drop: [22.2  2.  17.1] [4.9 9.2 3.7] [ 0. 41.  0.]\n",
      "cv_0_deg1_seq98_bis1  len 8473 %train/test/drop: [22.   2.5 17.1] [4.9 9.2 3.7] [ 0.  40.6  0. ]\n",
      "cv_1_deg1_seq98_bis1  len 10297 %train/test/drop: [22.2  2.  17.1] [4.9 9.2 3.7] [ 0. 41.  0.]\n",
      "cv_0_deg2_seq98  len 8473 %train/test/drop: [22.  43.1  1.3] [4.9 9.2 3.7] [ 0.   0.  15.8]\n",
      "cv_1_deg2_seq98  len 10297 %train/test/drop: [22.2 43.   1.1] [4.9 9.2 3.7] [ 0.  0. 16.]\n",
      "cv_0_deg2_seq98_bis1  len 8473 %train/test/drop: [22.  43.1  1.3] [4.9 9.2 3.7] [ 0.   0.  15.8]\n",
      "cv_1_deg2_seq98_bis1  len 10297 %train/test/drop: [22.2 43.   1.1] [4.9 9.2 3.7] [ 0.  0. 16.]\n"
     ]
    }
   ],
   "source": [
    "#Data_processor for preprocessed data stored in a dict\n",
    "from abench.benchmark import dataset_generator_from_array,splitter,TimeSeriesSplit,analyse_data_generator\n",
    "from Benchmark_UQ import Encapsulated_model_UQ,TimeSeries_from_dict\n",
    "from attack_function import ctx_seq_attack_UC_AL, ctx_seq_attack, test_attack\n",
    "\n",
    "# Synthetic data\n",
    "# Real data\n",
    "Folder_data = path + 'data/gas_demand_data/'\n",
    "Name_data = 'Dataset_real_data'\n",
    "Real_dataset = TimeSeries_from_dict(Folder_data+Name_data)\n",
    "Real_dataset.process()\n",
    "X,y,context,train,test, X_split = Real_dataset.get_data()\n",
    "X_shape,y_shape = X.shape[-1],y.shape[-1]\n",
    "X_train, X_test, y_train, y_test, context_train,context_test = Real_dataset.split_train_test()\n",
    "context[:,0] = context[:,2]*53 + context[:,3] - 13\n",
    "X_split = X_split-3\n",
    "X_split[X_split<0]=0\n",
    "sk_split=splitter(X_split)\n",
    "\n",
    "# Preliminary setup and subset partition recovery from data_generator\n",
    "def compute_ctx_var_clustering(y,context,n_ctx,class_number):\n",
    "    def in_list(idx,list_cluster):\n",
    "        for ind_cluster,list_idx in enumerate(list_cluster):\n",
    "            if(idx in list_idx):\n",
    "                return(ind_cluster)\n",
    "    std_values = [y[context[:,1]==i].std(axis=0).mean() for i in set(context[:,n_ctx])]\n",
    "    ind_sort = np.argsort(std_values)\n",
    "\n",
    "    ind_list=[]\n",
    "    for i,n in enumerate(ind_sort):\n",
    "        ind_list.append(n)\n",
    "\n",
    "    class_area_cat = np.array_split(ind_sort,class_number)\n",
    "    class_area_cat = [ind_sort[0:8],ind_sort[8:23],ind_sort[23:]]\n",
    "    class_area_ctx = [in_list(i,class_area_cat)for i in context[:,1]]\n",
    "    order_ctx = [ind_list.index(i) for i in context[:,1]]\n",
    "    new_context = np.concatenate([context,np.array(order_ctx)[:,None],np.array(class_area_ctx)[:,None]],axis=1)\n",
    "    return(new_context)\n",
    "context = compute_ctx_var_clustering(y,context,1,3)\n",
    "\n",
    "# Build data genarator\n",
    "objective = None\n",
    "dataset_generator = dataset_generator_from_array(X,y,context,objective,\n",
    "                                                 sk_split=sk_split,\n",
    "                                                 remove_from_train=None,\n",
    "                                                 repetition=2,\n",
    "                                                 attack_name='',\n",
    "                                                 cv_list_name=['cv_0','cv_1'])\n",
    "\n",
    "# Generation of training and inference variability injections\n",
    "if(True):\n",
    "    storing = path +'results/Benchmark_real_injection_learning'\n",
    "    if not os.path.exists(storing):\n",
    "        ratio_drop = 1 \n",
    "        for ctx_deg in range(3): # 100 withdrawal training injection dataset experiments for the 3 subsets\n",
    "            attack_str = '_deg'+str(ctx_deg)+'_seq'+str(int(ratio_drop*100))\n",
    "            remove_from_train_seq_ctx0 = ctx_seq_attack_UC_AL(y,context,ctx_deg,ratio_drop,\n",
    "                                                              len_past=12,len_futur=3,stick_seq=4)\n",
    "\n",
    "            dataset_generator_deg = dataset_generator_from_array(X,y,context,objective,\n",
    "                                                           sk_split=sk_split,\n",
    "                                                           repetition=2,\n",
    "                                                           remove_from_train=remove_from_train_seq_ctx0,\n",
    "                                                           attack_name=attack_str,\n",
    "                                                           cv_list_name=['cv_0','cv_1'])\n",
    "            dataset_generator += dataset_generator_deg\n",
    "            \n",
    "        sratio_drop = 0.98\n",
    "        for ctx_deg in range(3): # 98withdrawal training injection dataset experiments for the 3 subsets\n",
    "            attack_str = '_deg'+str(ctx_deg)+'_seq'+str(int(ratio_drop*100))\n",
    "            remove_from_train_seq_ctx0 = ctx_seq_attack_UC_AL(y,context,ctx_deg,ratio_drop,\n",
    "                                                              len_past=12,len_futur=3,stick_seq=4)\n",
    "\n",
    "            dataset_generator_deg = dataset_generator_from_array(X,y,context,objective,\n",
    "                                                           sk_split=sk_split,\n",
    "                                                           repetition=2,\n",
    "                                                           remove_from_train=remove_from_train_seq_ctx0,\n",
    "                                                           attack_name=attack_str,\n",
    "                                                           cv_list_name=['cv_0','cv_1'])\n",
    "            dataset_generator += dataset_generator_deg\n",
    "        print('Generation of data-generator at '+storing)\n",
    "        A_store.store_data_generator(storing,dataset_generator)\n",
    "    else:\n",
    "        dataset_generator=A_store.get_data_generator(storing)\n",
    "        print(storing+' already exists')\n",
    "if(False):\n",
    "    from src.attack_function import injection_on_data_generator,attack_bis\n",
    "    storing = path+'results/Benchmark_real_injection_inference'\n",
    "    list_f = [20,28,17,16,23] # Number of Main features contributions according to SHAP\n",
    "    type_f = ['Num','Num','Cat','Cat','Num','Num']\n",
    "    type_attack = [('small',2),('small',5),('strong',2),('strong',5)]\n",
    "    force = 0\n",
    "    if not os.path.exists(storing):\n",
    "        dataset_generator_with_attack = injection_on_data_generator(dataset_generator,type_attack,type_f,list_f,force)\n",
    "        A_store.store_data_generator(storing,dataset_generator_with_attack)\n",
    "    else:\n",
    "        dataset_generator = A_store.get_data_generator(storing)\n",
    "        print(storing+' already exists')\n",
    "\n",
    "list_cv_name_all = [i[-1] for i in dataset_generator]\n",
    "analyse_data_generator(dataset_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle Formalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest UQ\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from uqmodels.common.basic_NN import build_BNN_loss,build_EDL_loss,build_MSE_loss,default_callbacks,mlp\n",
    "\n",
    "step = 1000\n",
    "callbacks = default_callbacks(min_delta=0.005,earlystop_patience=60,reducelr_patience=30,reducelr_factor=0.4,reduce_lr_min_lr=5e-06,verbose=0)\n",
    "\n",
    "# MLP sans UQ\n",
    "type_var = None\n",
    "model_param={'dim_in':X_shape,'dim_out':y_shape, 'layers_size':[200,200,150],\n",
    "             'regularizer_W':(0.00005,0.00005),'name':'','dp':0.02,'type_var':type_var,\n",
    "             'logvar_min':np.log(0.00001)}\n",
    "\n",
    "training_param = {'epochs':[step,step,step],'b_s':[126,64,64],'l_r':[0.001,0.0005,0.0001],\n",
    "                  'sample_w':None,'callbacks':callbacks}\n",
    "\n",
    "\n",
    "MLP_parameters = {'rescale':False,\n",
    "                  'model_initializer':mlp,\n",
    "                  'model_parameters':model_param,\n",
    "                  'training_parameters':training_param,\n",
    "                  'type_var':type_var}\n",
    "\n",
    "# RF_dUQ\n",
    "RF_dUQ_parameters= {'estimator':RandomForestRegressor(ccp_alpha=5e-05, max_depth=18, max_features=0.8,\n",
    "                                                      max_samples=1., min_impurity_decrease=5e-05,\n",
    "                                                      min_samples_leaf=4,min_samples_split=6,\n",
    "                                                      n_estimators=50),\n",
    "                    'pretuned':False,'mode':'sigma','use_biais':True,'rescale':True}\n",
    "\n",
    "# PNN_MCDP\n",
    "type_var = 'MC_Dropout'\n",
    "model_param={'dim_in':X_shape,'dim_out':y_shape, 'layers_size':[200,300,200],\"n_ech\":6,\n",
    "             'regularizer_W':(0.0004,0.0004),'name':'','dp':0.25,'type_var':type_var,\n",
    "             'logvar_min':np.log(0.00001)}\n",
    "\n",
    "training_param = {'epochs':[step,step,step],'b_s':[128,32,64],'l_r':[0.001,0.0005,0.0001],\n",
    "                  'sample_w':None,'list_loss':[build_BNN_loss],\n",
    "                  'metrics':[build_MSE_loss(2,True),build_BNN_loss(0.9,True)],\n",
    "                  'param_loss':[1.05],'callbacks':callbacks}\n",
    "\n",
    "PNN_MCDP_parameters = {'rescale':False,\n",
    "                     'model_initializer':mlp,\n",
    "                     'model_parameters':model_param,\n",
    "                     'training_parameters':training_param,\n",
    "                     'type_var':type_var}\n",
    "# PNN_DE\n",
    "type_var = \"Deep_ensemble\"\n",
    "model_param={'dim_in':X_shape,'dim_out':y_shape, 'layers_size':[200,300,200],\n",
    "             'n_ech':5,'k_fold':8,\"snapshot\":False,'data_drop':0.05,'train_ratio':0.99,'ddof':1,\n",
    "             'regularizer_W':(0.001,0.001),'name':'','dp':0.03,'type_var':type_var,\n",
    "             'logvar_min':np.log(0.00001)}\n",
    "\n",
    "training_param = {'epochs':[step,step],'b_s':[128,64],'l_r':[0.001,0.0005],'sample_w':None,\n",
    "    'list_loss':[build_BNN_loss],'metrics':[build_MSE_loss(2,True),build_BNN_loss(0.9,True)],\n",
    "    'param_loss':[0.74],'callbacks':callbacks}\n",
    "\n",
    "\n",
    "PNN_DE_parameters = {'rescale':False,'model_initializer':mlp,'model_parameters':model_param.copy(),\n",
    "                     'training_parameters':training_param.copy(),'type_var':type_var}\n",
    "\n",
    "#EDL\n",
    "type_var='EDL'\n",
    "model_param={'dim_in':X_shape,'dim_out':y_shape, 'layers_size':[200,300,200],\n",
    "             'regularizer_W':(0.001,0.001),'name':'','dp':0.02,'type_var':type_var,\n",
    "             'logvar_min':np.log(0.00005)}\n",
    "\n",
    "\n",
    "training_param = {'epochs':[step,step,step],'b_s':[128,32,64],\n",
    "                  'l_r':[0.001,0.0005,0.0001],'sample_w':None,'list_loss':[build_EDL_loss],\n",
    "                  'metrics':[build_MSE_loss(4,True),build_BNN_loss(0.9,True,type_var)],\n",
    "                  'callbacks':callbacks,'param_loss':[15e-2]}\n",
    "              \n",
    "EDL_parameters = {'rescale':False,'model_initializer':mlp,'model_parameters':model_param.copy(),\n",
    "                  'training_parameters':training_param.copy(),'type_var':type_var}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation_studies parameters form mid-var with 100 withdrawal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_in_dict(dict_,dict_update_values):\n",
    "    for key in dict_update_values.keys():\n",
    "        dict_[key]=dict_update_values[key]\n",
    "        \n",
    "\n",
    "#PNN_MCDP very-low \n",
    "dict_update = {'regularizer_W':(0.00001,0.00001),'dp':0.02,\"n_ech\":3}\n",
    "replace_in_dict(model_param,dict_update)\n",
    "replace_in_dict(training_param,{'param_loss':[0.13]})\n",
    "PNN_MCDP_parameters_very_low = {'model_initializer':mlp, 'model_parameters':model_param.copy(),\n",
    "                                        'training_parameters':training_param.copy(),'type_var':type_var}\n",
    "#PNN_MCDP low \n",
    "dict_update = {'regularizer_W':(0.00001,0.00001),'dp':0.04,\"n_ech\":3}\n",
    "replace_in_dict(model_param,dict_update)\n",
    "replace_in_dict(training_param,{'param_loss':[0.15]})\n",
    "PNN_MCDP_parameters_low = {'model_initializer':mlp, 'model_parameters':model_param.copy(),\n",
    "                                        'training_parameters':training_param.copy(),'type_var':type_var}\n",
    "#PNN_MCDP high-low \n",
    "dict_update = {'regularizer_W':(0.001,0.001),'dp':0.40,\"n_ech\":6}\n",
    "replace_in_dict(model_param,dict_update)\n",
    "replace_in_dict(training_param,{'param_loss':[1.6]})\n",
    "PNN_MCDP_parameters_high = {'model_initializer':mlp, 'model_parameters':model_param.copy(),\n",
    "                                        'training_parameters':training_param.copy(),'type_var':type_var}\n",
    "#PNN_MCDP very-high\n",
    "dict_update = {'regularizer_W':(0.001,0.001),'dp':0.50,\"n_ech\":6}\n",
    "replace_in_dict(model_param,dict_update)\n",
    "replace_in_dict(training_param,{'param_loss':[1.8]})\n",
    "PNN_MCDP_parameters_very_high = {'model_initializer':mlp, 'model_parameters':model_param.copy(),\n",
    "                                        'training_parameters':training_param.copy(),'type_var':type_var}\n",
    "\n",
    "#PNN_DE very_low\n",
    "dict_update = {'n_ech':2,'k_fold':None,\"snapshot\":True,'data_drop':0,'train_ratio':0.95,\n",
    "               'regularizer_W':(0.00005,0.00005)}                                                                                                         \n",
    "replace_in_dict(model_param,dict_update)\n",
    "replace_in_dict(training_param,{'param_loss':[0.35]})\n",
    "PNN_DE_parameters_very_low = {'model_initializer':mlp,'model_parameters':model_param.copy(),\n",
    "                         'training_parameters':training_param.copy(),'type_var':type_var}\n",
    "\n",
    "#PNN_DE low \n",
    "dict_update = {'n_ech':2,'k_fold':None,\"snapshot\":True,'data_drop':0,'train_ratio':0.95,\n",
    "               'regularizer_W':(0.0004,0.0004)}                                                                                                         \n",
    "replace_in_dict(model_param,dict_update)\n",
    "replace_in_dict(training_param,{'param_loss':[0.55]})\n",
    "PNN_DE_parameters_low = {'model_initializer':mlp,'model_parameters':model_param.copy(),\n",
    "                         'training_parameters':training_param.copy(),'type_var':type_var}\n",
    "\n",
    "#PNN_DE high\n",
    "dict_update = {'n_ech':5,'k_fold':5,\"snapshot\":False,'data_drop':0.2,'train_ratio':0.99,\n",
    "               'regularizer_W':(0.003,0.003)}\n",
    "replace_in_dict(model_param,dict_update)\n",
    "replace_in_dict(training_param,{'param_loss':[0.7]})\n",
    "PNN_DE_parameters_high = {'model_initializer':mlp, 'model_parameters':model_param.copy(),\n",
    "                                        'training_parameters':training_param.copy(),'type_var':type_var}\n",
    "\n",
    "#PNN_DE very-high\n",
    "dict_update = {'n_ech':5,'k_fold':5,\"snapshot\":False,'data_drop':0.30,'train_ratio':0.99,\n",
    "               'regularizer_W':(0.005,0.005)}\n",
    "replace_in_dict(model_param,dict_update)\n",
    "replace_in_dict(training_param,{'param_loss':[0.6]})\n",
    "PNN_DE_parameters_very_high = {'model_initializer':mlp, 'model_parameters':model_param.copy(),\n",
    "                                        'training_parameters':training_param.copy(),'type_var':type_var}\n",
    "\n",
    "#PNN_DE RF_very_low\n",
    "RF_dUQ_parameters_very_low = {'estimator':RandomForestRegressor(ccp_alpha=1e-07, max_depth=25, max_features=0.8,\n",
    "                                                      max_samples=1., min_impurity_decrease=1e-07,\n",
    "                                                      min_samples_leaf=2, min_samples_split=2,\n",
    "                                                      n_estimators=12),\n",
    "                          'pretuned':False,'mode':'sigma','use_biais':True,'rescale':True}\n",
    "\n",
    "#RF_dUQ_low\n",
    "RF_dUQ_parameters_low = {'estimator':RandomForestRegressor(ccp_alpha=9e-06, max_depth=20, max_features=0.8,\n",
    "                                                      max_samples=1., min_impurity_decrease=9e-06,\n",
    "                                                      min_samples_leaf=2, min_samples_split=2,\n",
    "                                                      n_estimators=30),\n",
    "                     'pretuned':False,'mode':'sigma','use_biais':True,'rescale':True}\n",
    "\n",
    "#RF_dUQ_high\n",
    "RF_dUQ_parameters_high = {'estimator':RandomForestRegressor(ccp_alpha=1e-05, max_depth=15, max_features=0.8,\n",
    "                                                        max_samples=1., min_impurity_decrease=1e-05,\n",
    "                                                        min_samples_leaf=10,min_samples_split=10,\n",
    "                                                        n_estimators=50),\n",
    "                      'pretuned':False,'mode':'sigma','use_biais':True,'rescale':True}\n",
    "#RF_dUQ RF_very_high\n",
    "RF_dUQ_parameters_very_high = {'estimator':RandomForestRegressor(ccp_alpha=1e-05, max_depth=12,max_features=0.8,\n",
    "                                                                 max_samples=1., \n",
    "                                                             min_impurity_decrease=1e-05,\n",
    "                                                             min_samples_leaf=20,min_samples_split=20,\n",
    "                                                             n_estimators=50),\n",
    "                           'pretuned':False,'mode':'sigma','use_biais':True,'rescale':True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark formalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = np.random.randint(10000000)\n",
    "from uqmodels.common.neural_network_UQ import NN_var\n",
    "from uqmodels.common.random_forest_UQ import PredictorRF_UQ_distangle\n",
    "\n",
    "# Transmision des modèles / Initialisateurs de modèles\n",
    "dict_Predictor={}\n",
    "dict_Predictor['MLP'] = {'subpart':NN_var,'parameters':MLP_parameters}\n",
    "dict_Predictor['RF_dUQ'] = {'subpart':PredictorRF_UQ_distangle,'parameters':RF_dUQ_parameters}\n",
    "dict_Predictor['EDL'] = {'subpart':NN_var,'parameters':EDL_parameters}      \n",
    "dict_Predictor['PNN_DE'] = {'subpart':NN_var,'parameters':PNN_DE_parameters}\n",
    "dict_Predictor['PNN_MCDP'] = {'subpart':NN_var,'parameters':PNN_MCDP_parameters}\n",
    "\n",
    "exp_design = []\n",
    "# Spécification du plan d'expérience\n",
    "exp_design.append([{'name':'MLP_bis','model':'MLP'},\n",
    "                   {'name':'RF_dUQ_bis','model':'RF_dUQ'},\n",
    "                   {'name':'PNN_MCDP_bis','model':'PNN_MCDP'},\n",
    "                   {'name':'PNN_DE_bis','model':'PNN_DE'},\n",
    "                   {'name':'EDL_bis','model':'EDL'}])\n",
    "\n",
    "# Model wrapper :\n",
    "from src.Benchmark_UQ import Encapsulated_model_UQ\n",
    "# Configuration transmis à abench coté modèle\n",
    "dict_exp={'encapsulated_model': Encapsulated_model_UQ,\n",
    "          'tuning_scheme' : {'model':None},\n",
    "          'model' : dict_Predictor,\n",
    "          'exp_design':exp_design}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from src.Benchmark_UQ import rmse,average_coverage,sharpness,Gaussian_NLL\n",
    "from abench.benchmark import Generic_metric\n",
    "\n",
    "# Metrics definition to encapsulate in Generic_metric\n",
    "storing = path +'results/Benchmark_real_injection_learning'\n",
    "# or storing = path +'results/Benchmark_real_injection_inference'\n",
    "cv_list =  None\n",
    "\n",
    "from abench.benchmark import Generic_metric\n",
    "\n",
    "list_metrics=[Generic_metric(rmse,'Rmse_all', mask=None,list_ctx_constraint=None,reduce=True),\n",
    "              Generic_metric(average_coverage,\"Cov_ALL\", mask=None,list_ctx_constraint=None,reduce=True,type_var=\"all\"),\n",
    "              Generic_metric(Gaussian_NLL,\"NLL\", mask=None,list_ctx_constraint=None,reduce=True,type_var=\"all\"),\n",
    "              Generic_metric(sharpness,\"Aw\", mask=None,list_ctx_constraint=None,reduce=True,type_var=\"all\")]\n",
    "\n",
    "obj_param = {'alpha':0.05}\n",
    "tuning_kwarg = {}\n",
    "\n",
    "#Start from empty dict\n",
    "\n",
    "from abench.benchmark import benchmark\n",
    "dataset_generator = None\n",
    "# Run benchmark'Benchmark_UC_AL_0'\n",
    "storing = benchmark(storing,dataset_generator,dict_exp,obj_param,\n",
    "                    list_metrics,tuning_kwarg=tuning_kwarg,verbose=0,cv_list=cv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_0 len_train 6965\n",
      "cv_1 len_train 8473\n",
      "cv_0_bis1 len_train 6965\n",
      "cv_1_bis1 len_train 8473\n",
      "MLP  |time_fit : 216.82 time_pred : 2.22\n",
      "Rmse_all Train 0.151 ± 0.003 | TEST 0.216 ± 0.019\n",
      "NLL Train 1.271 ± 0.0 | TEST 1.277 ± 0.002\n",
      "Cov_all Train 1.0 ± 0.0 | TEST 1.0 ± 0.0\n",
      "Aw Train 5.644 ± 0.0 | TEST 5.644 ± 0.0\n",
      "\n",
      "RF_dUQ  |time_fit : 3.56 time_pred : 4.18\n",
      "Rmse_all Train 0.143 ± 0.003 | TEST 0.236 ± 0.015\n",
      "NLL Train -0.798 ± 0.015 | TEST -0.506 ± 0.053\n",
      "Cov_all Train 0.991 ± 0.001 | TEST 0.951 ± 0.01\n",
      "Aw Train 0.725 ± 0.009 | TEST 0.831 ± 0.01\n",
      "\n",
      "PNN_MCDP  |time_fit : 584.74 time_pred : 24.79\n",
      "Rmse_all Train 0.17 ± 0.004 | TEST 0.226 ± 0.017\n",
      "NLL Train -0.804 ± 0.017 | TEST -0.548 ± 0.075\n",
      "Cov_all Train 0.986 ± 0.001 | TEST 0.949 ± 0.013\n",
      "Aw Train 0.766 ± 0.012 | TEST 0.813 ± 0.007\n",
      "\n",
      "PNN_DE  |time_fit : 2036.57 time_pred : 5.98\n",
      "Rmse_all Train 0.153 ± 0.004 | TEST 0.211 ± 0.019\n",
      "NLL Train -0.848 ± 0.017 | TEST -0.581 ± 0.087\n",
      "Cov_all Train 0.99 ± 0.001 | TEST 0.952 ± 0.013\n",
      "Aw Train 0.702 ± 0.021 | TEST 0.73 ± 0.018\n",
      "\n",
      "EDL  |time_fit : 575.75 time_pred : 1.24\n",
      "Rmse_all Train 0.164 ± 0.008 | TEST 0.217 ± 0.014\n",
      "NLL Train -0.824 ± 0.029 | TEST -0.556 ± 0.087\n",
      "Cov_all Train 0.98 ± 0.001 | TEST 0.939 ± 0.016\n",
      "Aw Train 0.709 ± 0.028 | TEST 0.717 ± 0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from abench.benchmark import evaluate, Generic_metric\n",
    "from src.Benchmark_UQ import rmse,average_coverage,sharpness,Gaussian_NLL\n",
    "storing = path+'results/Benchmark_real_injection_learning'\n",
    "cv_list = ['cv_0','cv_1','cv_0_bis1','cv_1_bis1']\n",
    "list_name = ['MLP','RF_dUQ','PNN_MCDP','PNN_DE','EDL']\n",
    "list_ctx_constraint=None #[(-2,None,0.5)]\n",
    "list_metrics = [Generic_metric(rmse,'Rmse_all', mask=None,list_ctx_constraint=list_ctx_constraint,reduce=True),\n",
    "                Generic_metric(Gaussian_NLL,\"NLL\", mask=None,list_ctx_constraint=None,reduce=True),\n",
    "                Generic_metric(average_coverage,\"Cov_all\", mask=None,list_ctx_constraint=list_ctx_constraint,reduce=True),\n",
    "                Generic_metric(sharpness,\"Aw\", mask=None,list_ctx_constraint=list_ctx_constraint,reduce=True,type_var='all'),\n",
    "               ]\n",
    "dict_perf = evaluate(storing, list_name, list_metrics,verbose=1,cv_list=cv_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
